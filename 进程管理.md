### 进程管理

![process-os-detail](E:\Notes\rust\rcore_stage2\process-os-detail.png)

#### 进程管理的核心数据结构

为了更好实现进程管理，我们需要设计和调整内核中的一些数据结构，包括：

- 基于应用名的应用链接/加载器
- 进程标识符 `PidHandle` 以及内核栈 `KernelStack`
- 任务控制块 `TaskControlBlock`
- 任务管理器 `TaskManager`
- 处理器管理结构 `Processor`

##### 基于应用名的应用链接/加载器

在实现 `exec` 系统调用的时候，我们需要根据应用的名字而不仅仅是一个编号来获取应用的 ELF 格式数据。 因此，在链接器 `os/build.rs` 中，我们按顺序保存链接进来的每个应用的名字

各个应用的名字通过 `.string` 伪指令放到数据段中，注意链接器会自动在每个字符串的结尾加入分隔符 `\0` ，它们的位置由全局符号 `_app_names` 指出。

而在加载器 `loader.rs` 中，我们用一个全局可见的 *只读* 向量 `APP_NAMES` 来按照顺序将所有应用的名字保存在内存中：

##### 进程标识符和内核栈

**进程标识符**

同一时间存在的所有进程都有一个自己的进程标识符，它们是互不相同的整数。这里将其抽象为一个 `PidHandle` 类型，当它的生命周期结束后，对应的整数会被编译器自动回收：

```rust
// os/src/task/pid.rs

pub struct PidHandle(pub usize);
```

类似之前的物理页帧分配器 `FrameAllocator` ，我们实现一个同样使用简单栈式分配策略的进程标识符分配器 `PidAllocator` ，并将其全局实例化为 `PID_ALLOCATOR` ：

```rust
struct PidAllocator {
    current: usize,
    recycled: Vec<usize>,
}
```

从本章开始，我们将应用编号替换为进程标识符来决定每个进程内核栈在地址空间中的位置。

**内核栈**

在内核栈 `KernelStack` 中保存着它所属进程的 PID ：

```
// os/src/task/pid.rs

pub struct KernelStack {
    pid: usize,
}
```

btw: RAII（Resource Acquisition Is Initialization，资源获取即初始化）是C++语言中提出的一种管理资源的编程思想。RAII的核心思想是，将资源的分配和释放绑定到对象的生命周期上，即资源的获取和初始化在对象的创建过程中完成，资源的释放在对象的销毁过程中完成。这样，可以确保资源在使用过程中始终是有效的，并且在对象不再需要时资源会被自动释放，从而避免资源泄漏问题。

```rust
impl Drop for KernelStack {
    fn drop(&mut self) {
        let (kernel_stack_bottom, _) = kernel_stack_position(self.pid);
        let kernel_stack_bottom_va: VirtAddr = kernel_stack_bottom.into();
        KERNEL_SPACE
            .exclusive_access()
            .remove_area_with_start_vpn(kernel_stack_bottom_va.into());
    }
}
```

##### 进程控制块

承接前面的章节，我们仅需对任务控制块 `TaskControlBlock` 进行若干改动，让它直接承担进程控制块的功能：

```rust
 // os/src/task/task.rs
 
 pub struct TaskControlBlock {
     // immutable
     pub pid: PidHandle,
     pub kernel_stack: KernelStack,
     // mutable
     inner: UPSafeCell<TaskControlBlockInner>,
 }

pub struct TaskControlBlockInner {
    pub trap_cx_ppn: PhysPageNum,
    pub base_size: usize,
    pub task_cx: TaskContext,
    pub task_status: TaskStatus,
    pub memory_set: MemorySet,
    pub parent: Option<Weak<TaskControlBlock>>,
    pub children: Vec<Arc<TaskControlBlock>>,
    pub exit_code: i32,
}
```

`TaskControlBlockInner` 提供的方法主要是对于它内部字段的快捷访问：

##### 任务管理器

在前面的章节中，任务管理器 `TaskManager` 不仅负责管理所有的任务，还维护着 CPU 当前在执行哪个任务。 由于这种设计不够灵活，我们需要将任务管理器对于 CPU 的监控职能拆分到处理器管理结构 `Processor` 中去， **任务管理器自身仅负责管理所有任务。在这里，任务指的就是进程。**

```rust
 // os/src/task/manager.rs
 
 pub struct TaskManager {
     ready_queue: VecDeque<Arc<TaskControlBlock>>,
 }
 
 /// A simple FIFO scheduler.
 impl TaskManager {
     pub fn new() -> Self {
        Self {
            ready_queue: VecDeque::new(),
        }
    }
    pub fn add(&mut self, task: Arc<TaskControlBlock>) {
        self.ready_queue.push_back(task);
    }
    pub fn fetch(&mut self) -> Option<Arc<TaskControlBlock>> {
        self.ready_queue.pop_front()
    }
}

lazy_static! {
    pub static ref TASK_MANAGER: UPSafeCell<TaskManager> =
        unsafe { UPSafeCell::new(TaskManager::new()) };
}

pub fn add_task(task: Arc<TaskControlBlock>) {
    TASK_MANAGER.exclusive_access().add(task);
}

pub fn fetch_task() -> Option<Arc<TaskControlBlock>> {
    TASK_MANAGER.exclusive_access().fetch()
}
```

##### 处理器管理结构

处理器管理结构 `Processor` 负责维护从任务管理器 `TaskManager` 分离出去的那部分 CPU 状态：

```rust
// os/src/task/processor.rs

pub struct Processor {
    current: Option<Arc<TaskControlBlock>>,
    idle_task_cx: TaskContext,
}
```

btw: 来自GPT："idle控制流"（Idle control flow）通常指的是在嵌入式系统或其他实时系统中，当主程序或主要任务处于空闲状态时，CPU执行的控制流。这种控制流通常用于节省能源、等待中断或执行低优先级任务。闲置控制流通常是在主任务完成或没有可执行的高优先级任务时进入的一种状态。

##### 任务调度的 idle 控制流

每个 `Processor` 都有一个 idle 控制流，它们运行在每个核各自的启动栈上，功能是尝试从任务管理器中选出一个任务来在当前核上执行。 在内核初始化完毕之后，核通过调用 `run_tasks` 函数来进入 idle 控制流：

#### 进程管理机制的设计实现

- 初始进程 `initproc` 的创建；
- 进程调度机制：当进程主动调用 `sys_yield` 交出 CPU 使用权，或者内核本轮分配的时间片用尽之后如何切换到下一个进程；
- 进程生成机制：介绍进程相关的两个重要系统调用 `sys_fork/sys_exec` 的实现；
- 字符输入机制：介绍 `sys_read` 系统调用的实现；
- 进程资源回收机制：当进程调用 `sys_exit` 正常退出或者出错被内核终止后，如何保存其退出码，其父进程又是如何通过 `sys_waitpid` 收集该进程的信息并回收其资源。

##### 初始进程的创建

内核初始化完毕之后，即会调用 `task` 子模块提供的 `add_initproc` 函数来将初始进程 `initproc` 加入任务管理器，但在这之前，我们需要初始进程的进程控制块 `INITPROC` ，这基于 `lazy_static` 在运行时完成。

#### 进程的生成机制

##### **fork 系统调用**

实现 fork 时，最为关键且困难一点的是为子进程创建一个和父进程几乎完全相同的地址空间。我们的实现如下：

```rust
 // os/src/mm/memory_set.rs
 
 impl MapArea {
     pub fn from_another(another: &MapArea) -> Self {
         Self {
             vpn_range: VPNRange::new(
                 another.vpn_range.get_start(),
                 another.vpn_range.get_end()
             ),
              data_frames: BTreeMap::new(),
            map_type: another.map_type,
            map_perm: another.map_perm,
        }
    }
}

impl MemorySet {
    pub fn from_existed_user(user_space: &MemorySet) -> MemorySet {
        let mut memory_set = Self::new_bare();
        // map trampoline
        memory_set.map_trampoline();
        // copy data sections/trap_context/user_stack
        for area in user_space.areas.iter() {
        	let new_area = MapArea::from_another(area);
           	memory_set.push(new_area, None);
            // copy data from another space
            for vpn in area.vpn_range {
                let src_ppn = user_space.translate(vpn).unwrap().ppn();
                let dst_ppn = memory_set.translate(vpn).unwrap().ppn();
                dst_ppn.get_bytes_array().copy_from_slice(src_ppn.get_bytes_array());
            }
        }
        memory_set
    }
}
```

- 第 18 行的 `MemorySet::from_existed_user` 可以复制一个完全相同的地址空间。首先在第 19 行，我们通过 `new_bare` 新创建一个空的地址空间，并在第 21 行通过 `map_trampoline` 为这个地址空间映射上跳板页面，这是因为我们解析 ELF 创建地址空间的时候，并没有将跳板页作为一个单独的逻辑段插入到地址空间的逻辑段向量 `areas` 中，所以这里需要单独映射上。

  剩下的逻辑段都包含在 `areas` 中。我们遍历原地址空间中的所有逻辑段，将复制之后的逻辑段插入新的地址空间， 在插入的时候就已经实际分配了物理页帧了。接着我们遍历逻辑段中的每个虚拟页面，对应完成数据复制， 这只需要找出两个地址空间中的虚拟页面各被映射到哪个物理页帧，就可转化为将数据从物理内存中的一个位置复制到另一个位置，使用 `copy_from_slice` 即可轻松实现。

**接着，我们实现 `TaskControlBlock::fork` 来从父进程的进程控制块创建一份子进程的控制块**

- 子进程的地址空间不是通过解析 ELF，而是通过在第 8 行调用 `MemorySet::from_existed_user` 复制父进程地址空间得到的；
- 在 fork 的时候需要注意父子进程关系的维护。既要将父进程的弱引用计数放到子进程的进程控制块中，又要将子进程插入到父进程的孩子向量 `children` 中。

在调用 `sys_fork` 之前，我们已经将当前进程 Trap 上下文中的 sepc 向后移动了 4 字节，使得它回到用户态之后会从 ecall 的下一条指令开始执行。之后，当我们复制地址空间时，子进程地址空间 Trap 上下文的 sepc 也是移动之后的值，我们无需再进行修改。

父子进程回到用户态的瞬间都处于刚刚从一次系统调用返回的状态，但二者返回值不同。**第 8~11 行我们将子进程的 Trap 上下文中用来存放系统调用返回值的 a0 寄存器修改为 0 ，而父进程系统调用的返回值会在 `syscall` 返回之后再设置为 `sys_fork` 的返回值。**这就做到了父进程 `fork` 的返回值为子进程的 PID ，而子进程的返回值为 0。

##### **exec 系统调用的实现**

- 首先从 ELF 生成一个全新的地址空间并直接替换进来（第 15 行），这将导致原有地址空间生命周期结束，里面包含的全部物理页帧都会被回收；
- 然后修改新的地址空间中的 Trap 上下文，将解析得到的应用入口点、用户栈位置以及一些内核的信息进行初始化，这样才能正常实现 Trap 机制。

`sys_exec` 的实现如下，它调用 `translated_str` 找到要执行的应用名，并试图从应用加载器提供的 `get_app_data_by_name` 接口中获取对应的 ELF 数据，如果找到的话就调用 `TaskControlBlock::exec` 替换地址空间。

```rust
// os/src/syscall/process.rs

pub fn sys_exec(path: *const u8) -> isize {
    let token = current_user_token();
    let path = translated_str(token, path);
    if let Some(data) = get_app_data_by_name(path.as_str()) {
        let task = current_task().unwrap();
        task.exec(data);
        0
    } else {
        -1
    }
}
```

应用在 `sys_exec` 系统调用中传递给内核的只有一个应用名字符串在用户地址空间中的首地址，内核必限手动查页表来获得字符串的值。

`translated_str` 用来从用户地址空间中查找字符串，其原理就是逐字节查页表直到发现一个 `\0` 为止。为什么要逐字节查页表？ 因为内核不知道字符串的长度，且字符串可能是跨物理页的。

##### 系统调用后重新获取 Trap 上下文

#### 进程资源回收机制

当应用调用 `sys_exit` 系统调用主动退出，或者出错由内核终止之后，会在内核中调用 `exit_current_and_run_next` 函数：

 `exit_current_and_run_next` 带有一个退出码作为参数，这个退出码会在 `exit_current_and_run_next` 写入当前进程的进程控制块：

##### 父进程回收子进程资源